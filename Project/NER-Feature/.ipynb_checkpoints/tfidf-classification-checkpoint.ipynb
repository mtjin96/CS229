{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.naive_bayes as naive_bayes\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk.corpus\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english') + list(string.punctuation)\n",
    "def clean_article(s):\n",
    "    return [i for i in nltk.word_tokenize(s) if i not in stop_words]\n",
    "\n",
    "def load_data():\n",
    "    data = json.load(open('stories.json'))\n",
    "    for dataset in ['train', 'test', 'dev']:\n",
    "        for idx, article in enumerate(data[dataset]):\n",
    "            data[dataset][idx]['Content'] = clean_article(article['Content'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize input data\n",
    "corpus = [' '.join(article['Content']) for article in data['train']]\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TfidfVectorizer' object has no attribute 'vocabulary_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2a7d41f88d42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minv_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'TfidfVectorizer' object has no attribute 'vocabulary_'"
     ]
    }
   ],
   "source": [
    "inv_map = {v: k for k, v in vectorizer.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.721234021640768"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(['world trump donald'])[0, 5130]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-NN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_classes(data, t):\n",
    "    all_classes = set()\n",
    "    for typename in t:\n",
    "        articles = data[typename]\n",
    "        for article in articles:\n",
    "            all_classes = all_classes.union(set(article['Topics']))\n",
    "    return all_classes\n",
    "\n",
    "def get_classifier(classifier, vectorizer, data, t, topic_name):\n",
    "    articles = []\n",
    "    for type_name in t:\n",
    "        articles = articles + data[type_name]\n",
    "    \n",
    "    corpus = [' '.join(article['Content']) for article in articles]\n",
    "    mat_corpus = vectorizer.transform(corpus)\n",
    "    \n",
    "    y = np.asarray([topic_name in article['Topics'] for article in articles]).astype(int)\n",
    "    \n",
    "    classifier.fit(mat_corpus, y)\n",
    "    return classifier\n",
    "\n",
    "def get_result(classifier, data, t, topic_name):\n",
    "    articles = []\n",
    "    for type_name in t:\n",
    "        articles = articles + data[type_name]\n",
    "    \n",
    "    corpus = [' '.join(article['Content']) for article in articles]\n",
    "    mat_corpus = vectorizer.transform(corpus)\n",
    "    y = np.asarray([topic_name in article['Topics'] for article in articles]).astype(int)\n",
    "    \n",
    "    return classifier.predict(mat_corpus), y\n",
    "    \n",
    "all_classes = get_all_classes(data, ['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accidents and Natural Disasters 0.9840425531914894 0.9444444444444444 0.9638369598528961\n",
      "Arts and Entertainment 0.9946808510638298 0.5 0.6654804270462633\n",
      "Business 0.9361702127659575 0.2 0.3295880149812734\n",
      "Climate and Environment 0.9893617021276596 0.5 0.6642857142857143\n",
      "Context 0.9627659574468085 0.25 0.3969298245614035\n",
      "Economy 0.9308510638297872 0.5 0.6505576208178439\n",
      "Elections 0.9308510638297872 0.45 0.6067026194144838\n",
      "France 0.9946808510638298 0.5 0.6654804270462633\n",
      "Health and Medicine 0.9680851063829787 0.4 0.5660964230171073\n",
      "Immigration 0.9627659574468085 0.6666666666666666 0.7878128400435256\n",
      "Islamic State 0.973404255319149 0.16666666666666666 0.2846034214618973\n",
      "Law &amp; Justice 0.8085106382978724 0.631578947368421 0.7091757387247278\n",
      "North Korea 0.9840425531914894 0.8333333333333334 0.9024390243902439\n",
      "Politics 0.8138297872340425 0.625 0.7070240295748613\n",
      "Religion 0.973404255319149 0.16666666666666666 0.2846034214618973\n",
      "Russia 0.9468085106382979 0.7391304347826086 0.8301783264746228\n",
      "Science 0.9946808510638298 0.5 0.6654804270462633\n",
      "Sports 0.9787234042553191 0.2 0.3321299638989169\n",
      "Syria 0.9893617021276596 1.0 0.9946524064171123\n",
      "Technology 0.9787234042553191 0.625 0.7628524046434494\n",
      "Trump 0.8617021276595744 0.7 0.7724795640326976\n",
      "UK 0.9627659574468085 0.4 0.565183450429352\n",
      "War and Conflict 0.8936170212765957 0.675 0.7690742624618515\n",
      "World 0.8351063829787234 0.8735632183908046 0.853902028840305\n"
     ]
    }
   ],
   "source": [
    "for class_name in sorted(all_classes):\n",
    "    classifier = get_classifier(KNeighborsClassifier(n_neighbors=9), vectorizer, data, ['train', 'dev'], class_name)\n",
    "    predict_result = get_result(classifier, data, ['test'], class_name)\n",
    "    precision = 1 - np.sum(np.abs(predict_result[0] - predict_result[1])) / float(len(predict_result[0]))\n",
    "    result = [predict_result[0][i] == 1 and predict_result[1][i] == 1 for i in range(len(predict_result[0]))]\n",
    "    recall = (np.sum(result) + 1) / float(np.sum(predict_result[1]) + 1)\n",
    "    f1 = 2. / (1. / precision + 1. / recall)\n",
    "    print(class_name, precision, recall, f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accidents and Natural Disasters 0.9840425531914894 0.9444444444444444 0.9638369598528961\n",
      "Arts and Entertainment 0.9946808510638298 0.5 0.6654804270462633\n",
      "Business 0.9361702127659575 0.2 0.3295880149812734\n",
      "Climate and Environment 0.9893617021276596 0.5 0.6642857142857143\n",
      "Context 0.9627659574468085 0.25 0.3969298245614035\n",
      "Economy 0.9308510638297872 0.5 0.6505576208178439\n",
      "Elections 0.9308510638297872 0.45 0.6067026194144838\n",
      "France 0.9946808510638298 0.5 0.6654804270462633\n",
      "Health and Medicine 0.9680851063829787 0.4 0.5660964230171073\n",
      "Immigration 0.9627659574468085 0.6666666666666666 0.7878128400435256\n",
      "Islamic State 0.973404255319149 0.16666666666666666 0.2846034214618973\n",
      "Law &amp; Justice 0.8085106382978724 0.631578947368421 0.7091757387247278\n",
      "North Korea 0.9840425531914894 0.8333333333333334 0.9024390243902439\n",
      "Politics 0.8138297872340425 0.625 0.7070240295748613\n",
      "Religion 0.973404255319149 0.16666666666666666 0.2846034214618973\n",
      "Russia 0.9468085106382979 0.7391304347826086 0.8301783264746228\n",
      "Science 0.9946808510638298 0.5 0.6654804270462633\n",
      "Sports 0.9787234042553191 0.2 0.3321299638989169\n",
      "Syria 0.9893617021276596 1.0 0.9946524064171123\n",
      "Technology 0.9787234042553191 0.625 0.7628524046434494\n",
      "Trump 0.8617021276595744 0.7 0.7724795640326976\n",
      "UK 0.9627659574468085 0.4 0.565183450429352\n",
      "War and Conflict 0.8936170212765957 0.675 0.7690742624618515\n",
      "World 0.8351063829787234 0.8735632183908046 0.853902028840305\n"
     ]
    }
   ],
   "source": [
    "for class_name in sorted(all_classes):\n",
    "    classifier = get_classifier(KNeighborsClassifier(n_neighbors=9, metric='l2'), vectorizer, data, ['train', 'dev'], class_name)\n",
    "    predict_result = get_result(classifier, data, ['test'], class_name)\n",
    "    precision = 1 - np.sum(np.abs(predict_result[0] - predict_result[1])) / float(len(predict_result[0]))\n",
    "    result = [predict_result[0][i] == 1 and predict_result[1][i] == 1 for i in range(len(predict_result[0]))]\n",
    "    recall = (np.sum(result) + 1) / float(np.sum(predict_result[1]) + 1)\n",
    "    f1 = 2. / (1. / precision + 1. / recall)\n",
    "    print(class_name, precision, recall, f1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accidents and Natural Disasters 0.9574468085106383 0.6111111111111112 0.746043707611153\n",
      "Arts and Entertainment 0.9946808510638298 0.5 0.6654804270462633\n",
      "Business 0.9202127659574468 0.5333333333333333 0.675286655281776\n",
      "Climate and Environment 0.9680851063829787 0.25 0.39737991266375544\n",
      "Context 0.9521276595744681 0.5 0.6556776556776557\n",
      "Economy 0.9095744680851063 0.35 0.5054898648648648\n",
      "Elections 0.9202127659574468 0.45 0.6044254658385093\n",
      "France 0.9946808510638298 0.5 0.6654804270462633\n",
      "Health and Medicine 0.9361702127659575 0.1 0.18069815195071867\n",
      "Immigration 0.9414893617021276 0.5 0.6531365313653137\n",
      "Islamic State 0.973404255319149 0.16666666666666666 0.2846034214618973\n",
      "Law &amp; Justice 0.8297872340425532 0.7192982456140351 0.7706024096385543\n",
      "North Korea 0.9787234042553191 0.6666666666666666 0.793103448275862\n",
      "Politics 0.7925531914893618 0.6875 0.7362982929020665\n",
      "Religion 0.973404255319149 0.16666666666666666 0.2846034214618973\n",
      "Russia 0.9414893617021276 0.6956521739130435 0.800113010312191\n",
      "Science 0.9946808510638298 0.5 0.6654804270462633\n",
      "Sports 0.9787234042553191 0.2 0.3321299638989169\n",
      "Syria 0.9840425531914894 0.8 0.8825283243887894\n",
      "Technology 0.9521276595744681 0.375 0.5380761523046093\n",
      "Trump 0.8936170212765957 0.78 0.8329519450800915\n",
      "UK 0.9574468085106383 0.3 0.45685279187817257\n",
      "War and Conflict 0.8936170212765957 0.825 0.8579387186629526\n",
      "World 0.8351063829787234 0.896551724137931 0.8647389047770363\n"
     ]
    }
   ],
   "source": [
    "for class_name in sorted(all_classes):\n",
    "    classifier = get_classifier(naive_bayes.BernoulliNB(alpha=1), vectorizer, data, ['train', 'dev'], class_name)\n",
    "    predict_result = get_result(classifier, data, ['test'], class_name)\n",
    "    precision = 1 - np.sum(np.abs(predict_result[0] - predict_result[1])) / float(len(predict_result[0]))\n",
    "    result = [predict_result[0][i] == 1 and predict_result[1][i] == 1 for i in range(len(predict_result[0]))]\n",
    "    recall = (np.sum(result) + 1) / float(np.sum(predict_result[1]) + 1)\n",
    "    f1 = 2. / (1. / precision + 1. / recall)\n",
    "    print(class_name, precision, recall, f1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/yagami/lyl/cs229/env/local/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accidents and Natural Disasters 0.9095744680851063 0.05555555555555555 0.10471524800979792\n",
      "Arts and Entertainment 0.9946808510638298 0.5 0.6654804270462633\n",
      "Business 0.925531914893617 0.06666666666666667 0.12437455325232309\n",
      "Climate and Environment 0.9840425531914894 0.25 0.3987068965517241\n",
      "Context 0.9627659574468085 0.125 0.2212713936430318\n",
      "Economy 0.898936170212766 0.05 0.09473094170403587\n",
      "Elections 0.898936170212766 0.05 0.09473094170403587\n",
      "France 0.9946808510638298 0.5 0.6654804270462633\n",
      "Health and Medicine 0.9521276595744681 0.1 0.18099089989888775\n",
      "Immigration 0.9095744680851063 0.05555555555555555 0.10471524800979792\n",
      "Islamic State 0.973404255319149 0.16666666666666666 0.2846034214618973\n",
      "Law &amp; Justice 0.7021276595744681 0.017543859649122806 0.03423236514522822\n",
      "North Korea 0.973404255319149 0.16666666666666666 0.2846034214618973\n",
      "Politics 0.6648936170212766 0.015625 0.03053248656570591\n",
      "Religion 0.973404255319149 0.16666666666666666 0.2846034214618973\n",
      "Russia 0.8829787234042553 0.043478260869565216 0.08287568647029456\n",
      "Science 0.9946808510638298 0.5 0.6654804270462633\n",
      "Sports 0.9787234042553191 0.2 0.3321299638989169\n",
      "Syria 0.9787234042553191 0.2 0.3321299638989169\n",
      "Technology 0.9627659574468085 0.125 0.2212713936430318\n",
      "Trump 0.7393617021276595 0.02 0.03894648360885402\n",
      "UK 0.9521276595744681 0.1 0.18099089989888775\n",
      "War and Conflict 0.7925531914893618 0.025 0.048471047495120365\n",
      "World 0.5425531914893618 0.011494252873563218 0.022511586846170824\n"
     ]
    }
   ],
   "source": [
    "svm_classifier = SVC(kernel='poly', degree=4)\n",
    "for class_name in sorted(all_classes):\n",
    "    classifier = get_classifier(svm_classifier, vectorizer, data, ['train', 'dev'], class_name)\n",
    "    predict_result = get_result(classifier, data, ['test'], class_name)\n",
    "    precision = 1 - np.sum(np.abs(predict_result[0] - predict_result[1])) / float(len(predict_result[0]))\n",
    "    result = [predict_result[0][i] > 0.5 and predict_result[1][i] == 1 for i in range(len(predict_result[0]))]\n",
    "    recall = (np.sum(result) + 1) / float(np.sum(predict_result[1]) + 1)\n",
    "    f1 = 2. / (1. / precision + 1. / recall)\n",
    "    print(class_name, precision, recall, f1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accidents and Natural Disasters 0.9095744680851063 0.05555555555555555 0.10471524800979792\n",
      "Arts and Entertainment 0.9946808510638298 0.5 0.6654804270462633\n",
      "Business 0.925531914893617 0.06666666666666667 0.12437455325232309\n",
      "Climate and Environment 0.9840425531914894 0.25 0.3987068965517241\n",
      "Context 0.9627659574468085 0.125 0.2212713936430318\n",
      "Economy 0.898936170212766 0.05 0.09473094170403587\n",
      "Elections 0.898936170212766 0.05 0.09473094170403587\n",
      "France 0.9946808510638298 0.5 0.6654804270462633\n",
      "Health and Medicine 0.9521276595744681 0.1 0.18099089989888775\n",
      "Immigration 0.9095744680851063 0.05555555555555555 0.10471524800979792\n",
      "Islamic State 0.973404255319149 0.16666666666666666 0.2846034214618973\n",
      "Law &amp; Justice 0.7021276595744681 0.017543859649122806 0.03423236514522822\n",
      "North Korea 0.973404255319149 0.16666666666666666 0.2846034214618973\n",
      "Politics 0.6648936170212766 0.015625 0.03053248656570591\n",
      "Religion 0.973404255319149 0.16666666666666666 0.2846034214618973\n",
      "Russia 0.8829787234042553 0.043478260869565216 0.08287568647029456\n",
      "Science 0.9946808510638298 0.5 0.6654804270462633\n",
      "Sports 0.9787234042553191 0.2 0.3321299638989169\n",
      "Syria 0.9787234042553191 0.2 0.3321299638989169\n",
      "Technology 0.9627659574468085 0.125 0.2212713936430318\n",
      "Trump 0.7393617021276595 0.02 0.03894648360885402\n",
      "UK 0.9521276595744681 0.1 0.18099089989888775\n",
      "War and Conflict 0.7925531914893618 0.025 0.048471047495120365\n",
      "World 0.5425531914893618 0.011494252873563218 0.022511586846170824\n"
     ]
    }
   ],
   "source": [
    "svm_classifier = SVC(kernel='rbf')\n",
    "for class_name in sorted(all_classes):\n",
    "    classifier = get_classifier(svm_classifier, vectorizer, data, ['train', 'dev'], class_name)\n",
    "    predict_result = get_result(classifier, data, ['test'], class_name)\n",
    "    precision = 1 - np.sum(np.abs(predict_result[0] - predict_result[1])) / float(len(predict_result[0]))\n",
    "    result = [predict_result[0][i] == 1 and predict_result[1][i] == 1 for i in range(len(predict_result[0]))]\n",
    "    recall = (np.sum(result) + 1) / float(np.sum(predict_result[1]) + 1)\n",
    "    f1 = 2. / (1. / precision + 1. / recall)\n",
    "    print(class_name, precision, recall, f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
