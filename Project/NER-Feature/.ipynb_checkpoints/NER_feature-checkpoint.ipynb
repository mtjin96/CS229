{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.naive_bayes as naive_bayes\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk.corpus\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english') + list(string.punctuation)\n",
    "def clean_article(s):\n",
    "    return [i for i in nltk.word_tokenize(s) if i not in stop_words]\n",
    "\n",
    "def load_data():\n",
    "    data = json.load(open('stories.json'))\n",
    "    for dataset in ['train', 'test', 'dev']:\n",
    "        for idx, article in enumerate(data[dataset]):\n",
    "            data[dataset][idx]['Content'] = clean_article(article['Content'])\n",
    "            #data[dataset][idx]['Regions'] = article['Regions']\n",
    "            #data[dataset][idx]['Persons / Groups'] = article['Persons / Groups']\n",
    "            #data[dataset][idx]['Places'] = article['Places']\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()\n",
    "corpus = [' '.join(article['Content']) for article in data['train']]\n",
    "#words = nltk.word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word dictionary\n",
    "word_dict = {}\n",
    "idx = 0\n",
    "for cor in corpus:\n",
    "    token_lst = nltk.word_tokenize(cor)\n",
    "    word_lst_unq = set(token_lst)\n",
    "    for w in word_lst_unq:\n",
    "        if w not in word_dict:\n",
    "            word_dict[w] = idx\n",
    "            idx += 1\n",
    "            \n",
    "# create POS dictionary\n",
    "pos_dict = {'CC':0, 'CD':1, 'DT':2,'EX':3,'FW':4, 'IN':5, 'JJ':6,'JJR':7, 'JJS':8, 'LS':9,'MD':10,'NN':11, 'NNP':12, \n",
    "            'NNPS':13,'NNS':14,'PDT':15, 'POS':16,'PRP':17, 'PRP$':18,'RB':19, 'RBR':20, 'RBS':21, 'RP':22, 'SYM':23,\n",
    "            'TO':24, 'UH':25, 'VB':26, 'VBD':27, 'VBG':28,'VBN':29, 'VBP':30,'VBZ':31, 'WDT':32,'WP':33,'WP$':34, \n",
    "            'WRB':35, '``':36, '$':37,'\\'\\'':38, '(':39, ')':40, ',':41,'--':42,'.':43,':':44}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature vector (based on a token)\n",
    "window_sz = 2\n",
    "x = []\n",
    "for cor in corpus:\n",
    "    token_lst = nltk.word_tokenize(cor)\n",
    "    tags = nltk.pos_tag(token_lst)\n",
    "    for i in range(len(tags)):\n",
    "        if i >= window_sz and i < len(token_lst)-window_sz:\n",
    "            length = (2*window_sz+1)*len(word_dict)+(2*window_sz+1)*len(pos_dict)\n",
    "            feature = np.zeros(length)\n",
    "            idx = word_dict[tags[i][0]]\n",
    "            idx_pre1 = word_dict[tags[i-1][0]]\n",
    "            idx_pre2 = word_dict[tags[i-2][0]]\n",
    "            idx_aft1 = word_dict[tags[i+1][0]]\n",
    "            idx_aft2 = word_dict[tags[i+2][0]]\n",
    "            feature[idx] = 1\n",
    "            feature[idx_pre1+len(word_dict)*1] = 1\n",
    "            feature[idx_pre2+len(word_dict)*2] = 1\n",
    "            feature[idx_aft1+len(word_dict)*3] = 1\n",
    "            feature[idx_aft2+len(word_dict)*4] = 1\n",
    "            \n",
    "            idx_pos = pos_dict[tags[i][1]]\n",
    "            idx_pos_pre1 = pos_dict[tags[i-1][1]]\n",
    "            idx_pos_pre2 = pos_dict[tags[i-2][1]]\n",
    "            idx_pos_aft1 = pos_dict[tags[i+1][1]]\n",
    "            idx_pos_aft2 = pos_dict[tags[i+2][1]]\n",
    "            feature[idx_pos+(2*window_sz+1)*len(word_dict)] = 1\n",
    "            feature[idx_pos_pre1+len(pos_dict)*1+(2*window_sz+1)*len(word_dict)] = 1\n",
    "            feature[idx_pos_pre2+len(pos_dict)*2+(2*window_sz+1)*len(word_dict)] = 1\n",
    "            feature[idx_pos_aft1+len(pos_dict)*3+(2*window_sz+1)*len(word_dict)] = 1\n",
    "            feature[idx_pos_aft2+len(pos_dict)*4+(2*window_sz+1)*len(word_dict)] = 1\n",
    "            \n",
    "            \n",
    "            x.append(feature)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create set for all possible person words\n",
    "persons = [' '.join(article['Persons']) for article in data['train']]\n",
    "person = set()\n",
    "for i in persons:\n",
    "    per = nltk.word_tokenize(i)\n",
    "    for p in per:\n",
    "        if p not in person and (nltk.pos_tag([p])[0][1] in {'NNP', 'NN', 'NNS', 'JJ'}):\n",
    "            person.add(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels for each word\n",
    "y_per = []\n",
    "for cor in corpus:\n",
    "    token_lst = nltk.word_tokenize(cor)\n",
    "    for i in range(len(token_lst)):\n",
    "        if i >= 2 and i < len(token_lst)-2:\n",
    "            if token_lst[i] in person:\n",
    "                y_per.append(1)\n",
    "            else:\n",
    "                y_per.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146116"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "idx = 0\n",
    "token_lst = nltk.word_tokenize(\"The office of David Davis, Secretary of State\")\n",
    "for w in token_lst:\n",
    "    if w in person:\n",
    "        y.append(1)\n",
    "    else:\n",
    "        y.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, -1, -1, 1, 1, -1, 1, -1, 1]\n",
      "[-1, 1, 1, -1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(y[2:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "idx = 0\n",
    "token_lst = nltk.word_tokenize(\"The office of David Davis, Secretary of State\")\n",
    "word_lst_unq = set(token_lst)\n",
    "for w in word_lst_unq:\n",
    "    if w not in word_dict:\n",
    "        word_dict[w] = idx\n",
    "        idx += 1\n",
    "    \n",
    "x_t = []\n",
    "window_sz=2\n",
    "token_lst = nltk.word_tokenize(\"The office of David Davis, Secretary of State\")\n",
    "tags = nltk.pos_tag(token_lst)\n",
    "for i in range(len(tags)):\n",
    "    if i >= window_sz and i < len(token_lst)-window_sz:\n",
    "        length = (2*window_sz+1)*len(word_dict)+(2*window_sz+1)*len(pos_dict)\n",
    "        feature = np.zeros(length)\n",
    "        idx = word_dict[tags[i][0]]\n",
    "        idx_pre1 = word_dict[tags[i-1][0]]\n",
    "        idx_pre2 = word_dict[tags[i-2][0]]\n",
    "        idx_aft1 = word_dict[tags[i+1][0]]\n",
    "        idx_aft2 = word_dict[tags[i+2][0]]\n",
    "        feature[idx] = 1\n",
    "        feature[idx_pre1+len(word_dict)*1] = 1\n",
    "        feature[idx_pre2+len(word_dict)*2] = 1\n",
    "        feature[idx_aft1+len(word_dict)*3] = 1\n",
    "        feature[idx_aft2+len(word_dict)*4] = 1\n",
    "            \n",
    "        idx_pos = pos_dict[tags[i][1]]\n",
    "        idx_pos_pre1 = pos_dict[tags[i-1][1]]\n",
    "        idx_pos_pre2 = pos_dict[tags[i-2][1]]\n",
    "        idx_pos_aft1 = pos_dict[tags[i+1][1]]\n",
    "        idx_pos_aft2 = pos_dict[tags[i+2][1]]\n",
    "        feature[idx_pos+(2*window_sz+1)*len(word_dict)] = 1\n",
    "        feature[idx_pos_pre1+len(pos_dict)*1+(2*window_sz+1)*len(word_dict)] = 1\n",
    "        feature[idx_pos_pre2+len(pos_dict)*2+(2*window_sz+1)*len(word_dict)] = 1\n",
    "        feature[idx_pos_aft1+len(pos_dict)*3+(2*window_sz+1)*len(word_dict)] = 1\n",
    "        feature[idx_pos_aft2+len(pos_dict)*4+(2*window_sz+1)*len(word_dict)] = 1\n",
    "\n",
    "        x_t.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 1)\t0.46979138557992045\n",
      "  (1, 8)\t0.281088674033753\n",
      "  (1, 3)\t0.281088674033753\n",
      "  (1, 6)\t0.281088674033753\n",
      "  (1, 1)\t0.6876235979836938\n",
      "  (1, 5)\t0.5386476208856763\n",
      "  (2, 8)\t0.267103787642168\n",
      "  (2, 3)\t0.267103787642168\n",
      "  (2, 6)\t0.267103787642168\n",
      "  (2, 0)\t0.511848512707169\n",
      "  (2, 7)\t0.511848512707169\n",
      "  (2, 4)\t0.511848512707169\n",
      "  (3, 8)\t0.38408524091481483\n",
      "  (3, 3)\t0.38408524091481483\n",
      "  (3, 6)\t0.38408524091481483\n",
      "  (3, 2)\t0.5802858236844359\n",
      "  (3, 1)\t0.46979138557992045\n",
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "corpus = ['This is the first document.','This document is the second document.','And this is the third one.','Is this the first document?',]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
