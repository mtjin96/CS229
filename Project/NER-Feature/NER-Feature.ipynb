{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.svm import SVC\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk.corpus\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english') + list(string.punctuation)\n",
    "def clean_article(s):\n",
    "    return [i for i in nltk.word_tokenize(s) if i not in stop_words]\n",
    "\n",
    "def load_data():\n",
    "    data = json.load(open('stories.json'))\n",
    "    for dataset in ['train', 'test', 'dev']:\n",
    "        for idx, article in enumerate(data[dataset]):\n",
    "            data[dataset][idx]['Content'] = clean_article(article['Content'])\n",
    "            #data[dataset][idx]['Regions'] = article['Regions']\n",
    "            #data[dataset][idx]['Persons / Groups'] = article['Persons / Groups']\n",
    "            #data[dataset][idx]['Places'] = article['Places']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training dataset from CoNll2003\n",
    "train_content = np.genfromtxt('train_c.txt', dtype=str,usecols=(0, 3))\n",
    "\n",
    "train_data=[]\n",
    "label=[]\n",
    "i=-1\n",
    "\n",
    "for f in train_content:\n",
    "    if f[0] == '-DOCSTART-':\n",
    "        train_data.append([])\n",
    "        #label.append([])\n",
    "        i+=1\n",
    "    if f[0] != '-DOCSTART-':\n",
    "        train_data[i].append(f[0])\n",
    "        label.append(f[1])\n",
    "\n",
    "# load test dataset\n",
    "data =load_data()\n",
    "corpus = [' '.join(article['Content']) for article in data['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the two dataset\n",
    "data=[]\n",
    "y=[]\n",
    "for p in range(len(train_data)):\n",
    "    data.append(' '.join(train_data[p])) \n",
    "    y.append(label[p]) \n",
    "for a in range(len(corpus)): \n",
    "    data.append(corpus[a])\n",
    "\n",
    "# vectorize the whole text data\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, norm='l2')\n",
    "txt_vec = vectorizer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the feature vector for each word\n",
    "x = []\n",
    "#y_updated =[]\n",
    "for j in range(len(data)):\n",
    "    words_lst=nltk.word_tokenize(data[j])\n",
    "    for i in range(len(words_lst)):\n",
    "        length = (2*window_sz+1)*txt_vec.shape[1]\n",
    "        feature = np.zeros(length)\n",
    "        \n",
    "        if i>= 2 and i<len(words_lst)-2:\n",
    "            if vectorizer.vocabulary_.get(words_lst[i].lower()) != None:\n",
    "                idx = vectorizer.vocabulary_.get(words_lst[i].lower())\n",
    "                feature[idx] = txt_vec[j,idx]\n",
    "            \n",
    "            if vectorizer.vocabulary_.get(words_lst[i-1].lower()) != None:\n",
    "                idx_pre1 = vectorizer.vocabulary_.get((words_lst[i-1]).lower())\n",
    "                feature[idx_pre1+txt_vec.shape[1]*1] = txt_vec[j, idx_pre1]\n",
    "                \n",
    "            if  vectorizer.vocabulary_.get(words_lst[i-2].lower()) != None:  \n",
    "                idx_pre2 = vectorizer.vocabulary_.get((words_lst[i-2]).lower())\n",
    "                feature[idx_pre2+txt_vec.shape[1]*2] = txt_vec[j, idx_pre2]\n",
    "                \n",
    "            if vectorizer.vocabulary_.get(words_lst[i+1].lower()) != None: \n",
    "                idx_aft1 = vectorizer.vocabulary_.get((words_lst[i+1]).lower())\n",
    "                feature[idx_aft1+txt_vec.shape[1]*3] = txt_vec[j, idx_aft1]\n",
    "            \n",
    "            if vectorizer.vocabulary_.get(words_lst[i+2].lower()) != None:\n",
    "                idx_aft2 = vectorizer.vocabulary_.get((words_lst[i+2]).lower())\n",
    "                feature[idx_aft2+txt_vec.shape[1]*4] = txt_vec[j, idx_aft2]\n",
    "            \n",
    "            x.append(feature)\n",
    "            \n",
    "        elif i<2:\n",
    "            \n",
    "            if vectorizer.vocabulary_.get(words_lst[i].lower()) != None:\n",
    "                idx = vectorizer.vocabulary_.get(words_lst[i].lower())\n",
    "                feature[idx] = txt_vec[j,idx]\n",
    "            \n",
    "            if vectorizer.vocabulary_.get(words_lst[i+1].lower()) != None: \n",
    "                idx_aft1 = vectorizer.vocabulary_.get((words_lst[i+1]).lower())\n",
    "                feature[idx_aft1+txt_vec.shape[1]*3] = txt_vec[j, idx_aft1]\n",
    "            \n",
    "            if vectorizer.vocabulary_.get(words_lst[i+2].lower()) != None:\n",
    "                idx_aft2 = vectorizer.vocabulary_.get((words_lst[i+2]).lower())\n",
    "                feature[idx_aft2+txt_vec.shape[1]*4] = txt_vec[j, idx_aft2]\n",
    "            \n",
    "            x.append(feature)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            if vectorizer.vocabulary_.get(words_lst[i].lower()) != None:\n",
    "                idx = vectorizer.vocabulary_.get(words_lst[i].lower())\n",
    "                feature[idx] = txt_vec[j,idx]\n",
    "            \n",
    "            if vectorizer.vocabulary_.get(words_lst[i-1].lower()) != None:\n",
    "                idx_pre1 = vectorizer.vocabulary_.get((words_lst[i-1]).lower())\n",
    "                feature[idx_pre1+txt_vec.shape[1]*1] = txt_vec[j, idx_pre1]\n",
    "                \n",
    "            if  vectorizer.vocabulary_.get(words_lst[i-2].lower()) != None:  \n",
    "                idx_pre2 = vectorizer.vocabulary_.get((words_lst[i-2]).lower())\n",
    "                feature[idx_pre2+txt_vec.shape[1]*2] = txt_vec[j, idx_pre2]\n",
    "                \n",
    "            x.append(feature)\n",
    "\n",
    "x_feature=np.asarray(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit\n",
    "#classes = {'B-ORG', 'I-ORG', 'B-MISC','I-MISC','B-PER','I-PER','B-LOC', 'I-LOC', 'O'}\n",
    "svm_classifier = SVC(kernel='poly', degree=2)\n",
    "y=np.asarray(label)\n",
    "svm_classifier.fit(x_feature, y)\n",
    "X_test=x_feature[0:100]\n",
    "pre = svm_classifier.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
