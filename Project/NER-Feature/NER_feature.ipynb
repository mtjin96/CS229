{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'NERTagger'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-39b11f58dfbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstanford\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNERTagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'NERTagger'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.datasets import *\n",
    "import sklearn.naive_bayes as naive_bayes\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk.corpus\n",
    "import nltk\n",
    "import string\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tag.stanford import NERTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english') + list(string.punctuation)\n",
    "def clean_article(s):\n",
    "    return [i for i in nltk.word_tokenize(s) if i not in stop_words]\n",
    "\n",
    "def load_data():\n",
    "    data = json.load(open('stories.json'))\n",
    "    for dataset in ['train', 'test', 'dev']:\n",
    "        for idx, article in enumerate(data[dataset]):\n",
    "            data[dataset][idx]['Content'] = clean_article(article['Content'])\n",
    "            #data[dataset][idx]['Regions'] = article['Regions']\n",
    "            #data[dataset][idx]['Persons / Groups'] = article['Persons / Groups']\n",
    "            #data[dataset][idx]['Places'] = article['Places']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words=[]\n",
    "#for s in corpus:\n",
    "#    words.append(nltk.word_tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training dataset from CoNll2003\n",
    "train_content = np.genfromtxt('train_c.txt', dtype=str,usecols=(0, 3))\n",
    "\n",
    "train_data=[]\n",
    "label=[]\n",
    "i=-1\n",
    "#for f in train_content:\n",
    "#    if f[0]=='-DOCSTART-':\n",
    "#        train_data.append([])\n",
    "        #label.append([])\n",
    "#        i+=1\n",
    "#    if f[0] != '-DOCSTART-':\n",
    "#        train_data[i].append(f[0])\n",
    "#        label.append(f[1])\n",
    "\n",
    "for f in train_content:\n",
    "    if f[0] == '-DOCSTART-':\n",
    "        train_data.append([])\n",
    "        #label.append([])\n",
    "        i+=1\n",
    "    if f[0] != '-DOCSTART-':\n",
    "        train_data[i].append(f[0])\n",
    "        label.append(f[1])\n",
    "\n",
    "# load test dataset\n",
    "data =load_data()\n",
    "corpus = [' '.join(article['Content']) for article in data['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.21807542e-02  6.53760415e-03 -1.56343002e-02 -2.43179165e-02\n",
      "  3.32675166e-02 -8.14590454e-02 -2.18297560e-02 -1.31271228e-01\n",
      "  1.82046276e-03  9.59078372e-02  1.15824744e-01  3.65451761e-02\n",
      " -1.17703140e-01  1.24152467e-01  6.00628369e-02  9.54053327e-02\n",
      " -2.94515062e-02  8.36710334e-02  1.26652084e-02  2.28752978e-02\n",
      "  1.58165410e-01  5.42176217e-02  2.05847640e-02 -3.48507613e-02\n",
      "  2.93873195e-02  1.05917506e-01 -3.48369293e-02 -2.43419595e-02\n",
      " -4.49693464e-02 -8.28496665e-02  1.26450025e-02 -3.70780751e-02\n",
      "  4.41303588e-02 -8.25786665e-02  6.51074722e-02 -8.19406211e-02\n",
      "  4.98353504e-03  1.05181308e-02 -5.69385774e-02  2.64342353e-02\n",
      " -3.67391221e-02  1.10810958e-01  1.11701392e-01 -7.29196519e-02\n",
      " -2.09090393e-02 -4.04153112e-03 -2.64748354e-02 -8.80520269e-02\n",
      "  6.97187632e-02 -1.87883377e-02  2.00976655e-01  1.79003440e-02\n",
      " -1.11294590e-01  1.63668487e-02  1.86541323e-02 -1.33426040e-02\n",
      " -7.30426013e-02 -1.38386369e-01 -5.12950718e-02  7.02194422e-02\n",
      "  7.91218504e-02  7.60189742e-02  7.03237131e-02  6.54446706e-02\n",
      " -1.77369788e-02  5.04006147e-02  1.76525921e-01 -8.61081332e-02\n",
      "  7.03651235e-02  8.10123011e-02  1.19338125e-01  6.63162544e-02\n",
      " -6.38637021e-02 -9.69553217e-02 -3.94608676e-02 -2.26942878e-02\n",
      "  6.49352223e-02  1.31222149e-02  7.06661493e-03 -1.57733455e-01\n",
      "  3.26109268e-02  1.12689203e-02  1.92041602e-02  4.82629798e-02\n",
      " -4.97306138e-02  1.70615327e-04 -7.42914230e-02  2.48096157e-02\n",
      " -2.02995483e-02 -1.24148421e-01  6.27934784e-02  1.77350752e-02\n",
      " -2.50188857e-02  6.63855001e-02  8.64458531e-02 -1.82490498e-02\n",
      "  1.37999346e-02 -2.52228528e-02 -1.90219823e-02 -1.25920763e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinmengtian/Library/Python/3.6/lib/python/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(model['China'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinmengtian/Library/Python/3.6/lib/python/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "w_vec = []\n",
    "for n in range(len(train_data)):\n",
    "    for w2 in train_data[n]:\n",
    "        vec = model[w2]\n",
    "        w_vec.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8157"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=2, gamma='auto_deprecated',\n",
       "  kernel='poly', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_classifier = SVC(kernel='poly', degree=2)\n",
    "svm_classifier.fit(w_vec, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = [' '.join(article['Persons']) for article in data['test']]\n",
    "per=[]\n",
    "for p in person:\n",
    "    if p != '':\n",
    "        per.append(nltk.word_tokenize(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinmengtian/Library/Python/3.6/lib/python/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "p_vec = []\n",
    "for p in range(len(per)):\n",
    "    for p2 in per[p]:\n",
    "        if p2 in list(model.wv.vocab):\n",
    "            vec = model[p2]\n",
    "            p_vec.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O'], dtype='<U6')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_classifier.predict(p_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =load_data()\n",
    "corpus = [' '.join(article['Content']) for article in data['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = train_data\n",
    "for a in corpus:\n",
    "    sentence = nltk.word_tokenize(a)\n",
    "    sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the two dataset\n",
    "data=[]\n",
    "y=[]\n",
    "for p in range(len(train_data)):\n",
    "    data.append(' '.join(train_data[p])) \n",
    "    y.append(label[p]) \n",
    "for a in range(len(corpus)): \n",
    "    data.append(corpus[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the whole text data\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, norm='l2')\n",
    "txt_vec = vectorizer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['my', 'name', 'is', 'a', '.']"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx=vectorizer.vocabulary_.get('Trump'.lower())\n",
    "#vectorizer.get_feature_names()[19990:20000]\n",
    "#data\n",
    "txt_vec[0,idx]\n",
    "word_lst=['China', 'bye']\n",
    "idx=vectorizer.vocabulary_.get('\\'s')\n",
    "print(idx!=None)\n",
    "nltk.word_tokenize('my name is a.')\n",
    "#vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the feature vector for each word\n",
    "x = []\n",
    "#y_updated =[]\n",
    "for j in range(10):\n",
    "    words_lst=nltk.word_tokenize(data[j])\n",
    "    for i in range(len(words_lst)):\n",
    "        length = (2*window_sz+1)*txt_vec.shape[1]\n",
    "        feature = np.zeros(length)\n",
    "        \n",
    "        if i>= 2 and i<len(words_lst)-2:\n",
    "            if vectorizer.vocabulary_.get(words_lst[i].lower()) != None:\n",
    "                idx = vectorizer.vocabulary_.get(words_lst[i].lower())\n",
    "                feature[idx] = txt_vec[j,idx]\n",
    "            \n",
    "            if vectorizer.vocabulary_.get(words_lst[i-1].lower()) != None:\n",
    "                idx_pre1 = vectorizer.vocabulary_.get((words_lst[i-1]).lower())\n",
    "                feature[idx_pre1+txt_vec.shape[1]*1] = txt_vec[j, idx_pre1]\n",
    "                \n",
    "            if  vectorizer.vocabulary_.get(words_lst[i-2].lower()) != None:  \n",
    "                idx_pre2 = vectorizer.vocabulary_.get((words_lst[i-2]).lower())\n",
    "                feature[idx_pre2+txt_vec.shape[1]*2] = txt_vec[j, idx_pre2]\n",
    "                \n",
    "            if vectorizer.vocabulary_.get(words_lst[i+1].lower()) != None: \n",
    "                idx_aft1 = vectorizer.vocabulary_.get((words_lst[i+1]).lower())\n",
    "                feature[idx_aft1+txt_vec.shape[1]*3] = txt_vec[j, idx_aft1]\n",
    "            \n",
    "            if vectorizer.vocabulary_.get(words_lst[i+2].lower()) != None:\n",
    "                idx_aft2 = vectorizer.vocabulary_.get((words_lst[i+2]).lower())\n",
    "                feature[idx_aft2+txt_vec.shape[1]*4] = txt_vec[j, idx_aft2]\n",
    "            \n",
    "            x.append(feature)\n",
    "            \n",
    "        elif i<2:\n",
    "            \n",
    "            if vectorizer.vocabulary_.get(words_lst[i].lower()) != None:\n",
    "                idx = vectorizer.vocabulary_.get(words_lst[i].lower())\n",
    "                feature[idx] = txt_vec[j,idx]\n",
    "            \n",
    "            if vectorizer.vocabulary_.get(words_lst[i+1].lower()) != None: \n",
    "                idx_aft1 = vectorizer.vocabulary_.get((words_lst[i+1]).lower())\n",
    "                feature[idx_aft1+txt_vec.shape[1]*3] = txt_vec[j, idx_aft1]\n",
    "            \n",
    "            if vectorizer.vocabulary_.get(words_lst[i+2].lower()) != None:\n",
    "                idx_aft2 = vectorizer.vocabulary_.get((words_lst[i+2]).lower())\n",
    "                feature[idx_aft2+txt_vec.shape[1]*4] = txt_vec[j, idx_aft2]\n",
    "            \n",
    "            x.append(feature)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            if vectorizer.vocabulary_.get(words_lst[i].lower()) != None:\n",
    "                idx = vectorizer.vocabulary_.get(words_lst[i].lower())\n",
    "                feature[idx] = txt_vec[j,idx]\n",
    "            \n",
    "            if vectorizer.vocabulary_.get(words_lst[i-1].lower()) != None:\n",
    "                idx_pre1 = vectorizer.vocabulary_.get((words_lst[i-1]).lower())\n",
    "                feature[idx_pre1+txt_vec.shape[1]*1] = txt_vec[j, idx_pre1]\n",
    "                \n",
    "            if  vectorizer.vocabulary_.get(words_lst[i-2].lower()) != None:  \n",
    "                idx_pre2 = vectorizer.vocabulary_.get((words_lst[i-2]).lower())\n",
    "                feature[idx_pre2+txt_vec.shape[1]*2] = txt_vec[j, idx_pre2]\n",
    "                \n",
    "            x.append(feature)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feature=np.asarray(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2027, 105040)"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process testing data\n",
    "#context_test=[]\n",
    "#for p in range(100):\n",
    "#    for i in range(2,len(words[p])-2,1):\n",
    "        #print(train_data[p][i-2:i+3])\n",
    "#        context_test.append(' '.join(words[p][i-2:i+3]))\n",
    "\n",
    "# combine two dataset\n",
    "#for i in context_test:\n",
    "#    context.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize data\n",
    "#vectorizer = TfidfVectorizer(sublinear_tf=True, norm='l2')\n",
    "#X = vectorizer.fit_transform(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O'}"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes=set(label)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=2, gamma='auto_deprecated',\n",
       "  kernel='poly', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit\n",
    "#classes = {'B-ORG', 'I-ORG', 'B-MISC','I-MISC','B-PER','I-PER','B-LOC', 'I-LOC', 'O'}\n",
    "svm_classifier = SVC(kernel='poly', degree=2)\n",
    "y=np.asarray(label[0:x_feature.shape[0]])\n",
    "svm_classifier.fit(x_feature, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_feature[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], dtype='<U6')"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict\n",
    "X_test=x_feature[0:100]\n",
    "pre = svm_classifier.predict(X_test)\n",
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word dictionary\n",
    "word_dict = {}\n",
    "idx = 0\n",
    "for cor in corpus:\n",
    "    token_lst = nltk.word_tokenize(cor)\n",
    "    word_lst_unq = set(token_lst)\n",
    "    for w in word_lst_unq:\n",
    "        if w not in word_dict:\n",
    "            word_dict[w] = idx\n",
    "            idx += 1\n",
    "            \n",
    "# create POS dictionary\n",
    "pos_dict = {'CC':0, 'CD':1, 'DT':2,'EX':3,'FW':4, 'IN':5, 'JJ':6,'JJR':7, 'JJS':8, 'LS':9,'MD':10,'NN':11, 'NNP':12, \n",
    "            'NNPS':13,'NNS':14,'PDT':15, 'POS':16,'PRP':17, 'PRP$':18,'RB':19, 'RBR':20, 'RBS':21, 'RP':22, 'SYM':23,\n",
    "            'TO':24, 'UH':25, 'VB':26, 'VBD':27, 'VBG':28,'VBN':29, 'VBP':30,'VBZ':31, 'WDT':32,'WP':33,'WP$':34, \n",
    "            'WRB':35, '``':36, '$':37,'\\'\\'':38, '(':39, ')':40, ',':41,'--':42,'.':43,':':44}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature vector (based on a token)\n",
    "window_sz = 2\n",
    "x = []\n",
    "for cor in corpus:\n",
    "    token_lst = nltk.word_tokenize(cor)\n",
    "    tags = nltk.pos_tag(token_lst)\n",
    "    for i in range(len(tags)):\n",
    "        if i >= window_sz and i < len(token_lst)-window_sz:\n",
    "            length = (2*window_sz+1)*len(word_dict)+(2*window_sz+1)*len(pos_dict)\n",
    "            feature = np.zeros(length)\n",
    "            idx = word_dict[tags[i][0]]\n",
    "            idx_pre1 = word_dict[tags[i-1][0]]\n",
    "            idx_pre2 = word_dict[tags[i-2][0]]\n",
    "            idx_aft1 = word_dict[tags[i+1][0]]\n",
    "            idx_aft2 = word_dict[tags[i+2][0]]\n",
    "            feature[idx] = 1\n",
    "            feature[idx_pre1+len(word_dict)*1] = 1\n",
    "            feature[idx_pre2+len(word_dict)*2] = 1\n",
    "            feature[idx_aft1+len(word_dict)*3] = 1\n",
    "            feature[idx_aft2+len(word_dict)*4] = 1\n",
    "            \n",
    "            idx_pos = pos_dict[tags[i][1]]\n",
    "            idx_pos_pre1 = pos_dict[tags[i-1][1]]\n",
    "            idx_pos_pre2 = pos_dict[tags[i-2][1]]\n",
    "            idx_pos_aft1 = pos_dict[tags[i+1][1]]\n",
    "            idx_pos_aft2 = pos_dict[tags[i+2][1]]\n",
    "            feature[idx_pos+(2*window_sz+1)*len(word_dict)] = 1\n",
    "            feature[idx_pos_pre1+len(pos_dict)*1+(2*window_sz+1)*len(word_dict)] = 1\n",
    "            feature[idx_pos_pre2+len(pos_dict)*2+(2*window_sz+1)*len(word_dict)] = 1\n",
    "            feature[idx_pos_aft1+len(pos_dict)*3+(2*window_sz+1)*len(word_dict)] = 1\n",
    "            feature[idx_pos_aft2+len(pos_dict)*4+(2*window_sz+1)*len(word_dict)] = 1\n",
    "            \n",
    "            \n",
    "            x.append(feature)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create set for all possible person words\n",
    "persons = [' '.join(article['Persons']) for article in data['train']]\n",
    "person = set()\n",
    "for i in persons:\n",
    "    per = nltk.word_tokenize(i)\n",
    "    for p in per:\n",
    "        if p not in person and (nltk.pos_tag([p])[0][1] in {'NNP', 'NN', 'NNS', 'JJ'}):\n",
    "            person.add(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels for each word\n",
    "y_per = []\n",
    "for cor in corpus:\n",
    "    token_lst = nltk.word_tokenize(cor)\n",
    "    for i in range(len(token_lst)):\n",
    "        if i >= 2 and i < len(token_lst)-2:\n",
    "            if token_lst[i] in person:\n",
    "                y_per.append(1)\n",
    "            else:\n",
    "                y_per.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146116"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "idx = 0\n",
    "token_lst = nltk.word_tokenize(\"The office of David Davis, Secretary of State\")\n",
    "for w in token_lst:\n",
    "    if w in person:\n",
    "        y.append(1)\n",
    "    else:\n",
    "        y.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, -1, -1, 1, 1, -1, 1, -1, 1]\n",
      "[-1, 1, 1, -1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(y[2:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "idx = 0\n",
    "token_lst = nltk.word_tokenize(\"The office of David Davis, Secretary of State\")\n",
    "word_lst_unq = set(token_lst)\n",
    "for w in word_lst_unq:\n",
    "    if w not in word_dict:\n",
    "        word_dict[w] = idx\n",
    "        idx += 1\n",
    "    \n",
    "x_t = []\n",
    "window_sz=2\n",
    "token_lst = nltk.word_tokenize(\"The office of David Davis, Secretary of State\")\n",
    "tags = nltk.pos_tag(token_lst)\n",
    "for i in range(len(tags)):\n",
    "    if i >= window_sz and i < len(token_lst)-window_sz:\n",
    "        length = (2*window_sz+1)*len(word_dict)+(2*window_sz+1)*len(pos_dict)\n",
    "        feature = np.zeros(length)\n",
    "        idx = word_dict[tags[i][0]]\n",
    "        idx_pre1 = word_dict[tags[i-1][0]]\n",
    "        idx_pre2 = word_dict[tags[i-2][0]]\n",
    "        idx_aft1 = word_dict[tags[i+1][0]]\n",
    "        idx_aft2 = word_dict[tags[i+2][0]]\n",
    "        feature[idx] = 1\n",
    "        feature[idx_pre1+len(word_dict)*1] = 1\n",
    "        feature[idx_pre2+len(word_dict)*2] = 1\n",
    "        feature[idx_aft1+len(word_dict)*3] = 1\n",
    "        feature[idx_aft2+len(word_dict)*4] = 1\n",
    "            \n",
    "        idx_pos = pos_dict[tags[i][1]]\n",
    "        idx_pos_pre1 = pos_dict[tags[i-1][1]]\n",
    "        idx_pos_pre2 = pos_dict[tags[i-2][1]]\n",
    "        idx_pos_aft1 = pos_dict[tags[i+1][1]]\n",
    "        idx_pos_aft2 = pos_dict[tags[i+2][1]]\n",
    "        feature[idx_pos+(2*window_sz+1)*len(word_dict)] = 1\n",
    "        feature[idx_pos_pre1+len(pos_dict)*1+(2*window_sz+1)*len(word_dict)] = 1\n",
    "        feature[idx_pos_pre2+len(pos_dict)*2+(2*window_sz+1)*len(word_dict)] = 1\n",
    "        feature[idx_pos_aft1+len(pos_dict)*3+(2*window_sz+1)*len(word_dict)] = 1\n",
    "        feature[idx_pos_aft2+len(pos_dict)*4+(2*window_sz+1)*len(word_dict)] = 1\n",
    "\n",
    "        x_t.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46941728 0.61722732 0.3645444  0.         0.\n",
      "  0.3645444  0.         0.3645444 ]\n",
      " [0.         0.7284449  0.         0.28285122 0.         0.47890875\n",
      "  0.28285122 0.         0.28285122]\n",
      " [0.49711994 0.         0.         0.29360705 0.49711994 0.\n",
      "  0.29360705 0.49711994 0.29360705]]\n",
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "corpus = ['This is the first document.','This document is the second document.','And this is the third one.']\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_.get('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "sentence = \"At eight o'clock on Thursday morning... Arthur didn't feel very good.\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "entities = nltk.chunk.ne_chunk(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[]\n",
    "for a in corpus:\n",
    "    for sent in nltk.sent_tokenize(a):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label'):\n",
    "                res=(chunk.label(), ' '.join(c[0] for c in chunk))\n",
    "                result.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [' '.join(article['Content']) for article in data['test']]\n",
    "corpus2 = [' '.join(article['Content']) for article in data['train']]\n",
    "corpus3 = [' '.join(article['Content']) for article in data['train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('stories.csv', sep=',')\n",
    "df=df.dropna()\n",
    "df=df.reset_index(drop='True')\n",
    "df.columns.values\n",
    "content = df['Content no HTML']\n",
    "num_news = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER={'Persons / Groups': set(), 'ORGANIZATION': set(), 'GPE':set(), 'LOCATION':set(), 'FACILITY':set(), 'GSP':set()}\n",
    "Origin={'Persons / Groups': set(), 'Regions': set(), 'Events':set(), 'Places':set()}\n",
    "for key in Origin.keys():\n",
    "    df[key]=df[key].str.split('|')\n",
    "    for i in range(len(df)):\n",
    "        Origin[key]=Origin[key]|set(df[key][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in result:\n",
    "    if item[0] == 'PERSON':\n",
    "        NER['Persons / Groups'].add(item[1])\n",
    "    if item[0] == 'ORGANIZATION':\n",
    "        NER['ORGANIZATION'].add(item[1])\n",
    "    if item[0] == 'GPE':\n",
    "        NER['GPE'].add(item[1])\n",
    "    if item[0] == 'LOCATION':\n",
    "        NER['LOCATION'].add(item[1])\n",
    "    if item[0] == 'FACILITY':\n",
    "        NER['FACILITY'].add(item[1])\n",
    "    if item[0] == 'FACILITY':\n",
    "        NER['GSP'].add(item[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_1=pd.DataFrame(index=NER.keys(),columns=Origin.keys())\n",
    "similarity_2=pd.DataFrame(index=NER.keys(),columns=Origin.keys())\n",
    "for i in Origin.keys():\n",
    "    for j in NER.keys():\n",
    "        similarity_1[i][j]=len(Origin[i]&NER[j])/len(Origin[i])\n",
    "        \n",
    "for i in Origin.keys():\n",
    "    for j in NER.keys():\n",
    "        similarity_2[i][j]=len(Origin[i]&NER[j])/len(Origin[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Persons / Groups</th>\n",
       "      <th>Regions</th>\n",
       "      <th>Events</th>\n",
       "      <th>Places</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Persons / Groups</th>\n",
       "      <td>0.238854</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.00714286</td>\n",
       "      <td>0.311111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <td>0.124204</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0</td>\n",
       "      <td>0.103704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPE</th>\n",
       "      <td>0.0191083</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>0.362963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOCATION</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FACILITY</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSP</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Persons / Groups   Regions      Events    Places\n",
       "Persons / Groups         0.238854  0.444444  0.00714286  0.311111\n",
       "ORGANIZATION             0.124204  0.111111           0  0.103704\n",
       "GPE                     0.0191083  0.666667           0  0.362963\n",
       "LOCATION                        0         0           0  0.037037\n",
       "FACILITY                        0         0           0         0\n",
       "GSP                             0         0           0         0"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Persons / Groups</th>\n",
       "      <th>Regions</th>\n",
       "      <th>Events</th>\n",
       "      <th>Places</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Persons / Groups</th>\n",
       "      <td>0.238854</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.00714286</td>\n",
       "      <td>0.311111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <td>0.124204</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0</td>\n",
       "      <td>0.103704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPE</th>\n",
       "      <td>0.0191083</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>0.362963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOCATION</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FACILITY</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSP</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Persons / Groups   Regions      Events    Places\n",
       "Persons / Groups         0.238854  0.444444  0.00714286  0.311111\n",
       "ORGANIZATION             0.124204  0.111111           0  0.103704\n",
       "GPE                     0.0191083  0.666667           0  0.362963\n",
       "LOCATION                        0         0           0  0.037037\n",
       "FACILITY                        0         0           0         0\n",
       "GSP                             0         0           0         0"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Persons / Groups    0.382166\n",
       "Regions             1.222222\n",
       "Events              0.007143\n",
       "Places              0.814815\n",
       "dtype: float64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(similarity_1,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FACILITY', 'GPE', 'GSP', 'LOCATION', 'ORGANIZATION', 'PERSON'}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
